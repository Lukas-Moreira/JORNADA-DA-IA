# Cap√≠tulo 2: Fundamentos de Redes Neurais üß†

Este cap√≠tulo aborda os conceitos essenciais das redes neurais, explicando desde sua estrutura at√© o funcionamento de suas fun√ß√µes internas e ativa√ß√£o. Tamb√©m inclui um exemplo pr√°tico de implementa√ß√£o.

---

## üìå O que √© uma Rede Neural?

Uma rede neural √© uma estrutura computacional inspirada no c√©rebro humano, composta por unidades chamadas **neur√¥nios artificiais**. Seu objetivo √© processar e transmitir informa√ß√µes por meio de conex√µes com pesos ajust√°veis.

Esse processo √© conhecido como **aprendizado por representa√ß√£o**, onde os dados atravessam m√∫ltiplas camadas e a rede aprende **padr√µes complexos**.

---

## üß± Estrutura de uma Rede Neural

### 1. Camada de Entrada
Recebe os dados iniciais (imagem, texto, s√©rie temporal). Cada n√≥ representa uma **caracter√≠stica (feature)** dos dados.

### 2. Camadas Ocultas
Onde ocorre o **processamento dos dados**. Cada neur√¥nio aplica transforma√ß√µes aos dados com base em pesos e fun√ß√µes de ativa√ß√£o. A quantidade de camadas e neur√¥nios define a profundidade da rede.

### 3. Camada de Sa√≠da
Gera o **resultado final da rede**. Em classifica√ß√£o, cada neur√¥nio representa uma **classe poss√≠vel**.

---

## üîó Conex√µes e Pesos

Cada conex√£o entre neur√¥nios tem um **peso**, que determina sua import√¢ncia. Durante o treinamento, esses pesos s√£o ajustados para melhorar o desempenho da rede.

---

## ‚û°Ô∏è Propaga√ß√£o Direta (Forward Propagation)

√â o processo de transmiss√£o de dados da entrada at√© a sa√≠da, b√°sicamente aqui √© onde a soma ponderada que os neur√¥nios da camada oculta realizam ap√≥s receberem a os dados da camada anterior, para assim aplicar a fun√ß√£o de ativa√ß√£o que √© respons√°vel por gerar a sa√≠da.

### F√≥rmula do Neur√¥nio

**z = w1 * x1 + w2 * x2 + ... + wn * xn + b**

Onde:
- `x` = entradas
- `w` = pesos
- `b` = vi√©s
- `z` = valor de ativa√ß√£o antes da fun√ß√£o de ativa√ß√£o

---

## ‚öôÔ∏è Fun√ß√µes de Ativa√ß√£o

As fun√ß√µes de ativa√ß√£o introduzem **n√£o linearidade** na rede. Principais tipos:

- `Sigmoid` ‚Üí Sa√≠da entre 0 e 1 (classifica√ß√£o bin√°ria)
- `Tanh`    ‚Üí Sa√≠da entre -1 e 1 (centraliza dados)
- `ReLU`    ‚Üí Zera valores negativos, eficiente em redes profundas
- `Softmax` ‚Üí Converte em distribui√ß√£o de probabilidade (multi-classes)

---

## üí° Exemplo Pr√°tico (`main.py`)

Este cap√≠tulo inclui uma rede neural com:
- **Camada oculta com 4 neur√¥nios (ReLU)**
- **Camada de sa√≠da com 1 neur√¥nio (Sigmoid)**

Essa rede simula o comportamento de uma **porta l√≥gica XOR**, onde a sa√≠da √© `1` quando as entradas s√£o diferentes.

---

üìÅ *Este conte√∫do faz parte da minha jornada de estudos em IA, com experimentos pr√°ticos implementados em Python e frameworks como TensorFlow.*
